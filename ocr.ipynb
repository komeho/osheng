{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ocr.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN3kcHstxnSmJSqO3Fdyw1C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"nAfRYOe9ZXgr"},"source":["How to [source](https://towardsdatascience.com/extracting-text-from-scanned-pdf-using-pytesseract-open-cv-cd670ee38052)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCZtVoVcFTrH","executionInfo":{"status":"ok","timestamp":1616429884032,"user_tz":-480,"elapsed":21051,"user":{"displayName":"Axel Mukwena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkBUMeSLjKE8qwDBLKGkDOZkIiuxwvNO8t-yPMcPM=s64","userId":"13593993050630757383"}},"outputId":"4311c4b6-c614-4212-f595-7cbc557a5f26"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZxDp2ND-FdRy","executionInfo":{"status":"ok","timestamp":1616429888194,"user_tz":-480,"elapsed":2131,"user":{"displayName":"Axel Mukwena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkBUMeSLjKE8qwDBLKGkDOZkIiuxwvNO8t-yPMcPM=s64","userId":"13593993050630757383"}},"outputId":"c010583c-ea5d-404a-f8d9-d1ecc234adb6"},"source":["import os\n","os.chdir(\"/content/drive/MyDrive/offline/projects/ml/oshengResources/\")\n","!ls"],"execution_count":5,"outputs":[{"output_type":"stream","text":[" 100.pdf\t       dummy.pdf   oshengBook.pdf   processed_pages\n","'Copy of page_1.jpg'   ocr.ipynb   pages\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"O_ZO7kZ3Oejk","executionInfo":{"status":"ok","timestamp":1616429949830,"user_tz":-480,"elapsed":37185,"user":{"displayName":"Axel Mukwena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkBUMeSLjKE8qwDBLKGkDOZkIiuxwvNO8t-yPMcPM=s64","userId":"13593993050630757383"}},"outputId":"c3b43d73-9b9b-472a-9f6a-6c9cb8e3f2cf"},"source":["# https://nanonets.com/blog/ocr-with-tesseract/#whitelistingcharacters\n","!pip install pdf2image\n","!sudo apt install tesseract-ocr\n","!pip install pytesseract\n","!pip install tesseract\n","# https://colab.research.google.com/drive/10doc9xwhFDpDGNferehBzkQ6M0Un-tYq#scrollTo=TtkG0J2Emfgl\n","!apt-get install poppler-utils"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Collecting pdf2image\n","  Downloading https://files.pythonhosted.org/packages/03/62/089030fd16ab3e5c245315d63c80b29250b8f9e4579b5a09306eb7e7539c/pdf2image-1.14.0-py3-none-any.whl\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pdf2image) (7.0.0)\n","Installing collected packages: pdf2image\n","Successfully installed pdf2image-1.14.0\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  tesseract-ocr-eng tesseract-ocr-osd\n","The following NEW packages will be installed:\n","  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n","0 upgraded, 3 newly installed, 0 to remove and 30 not upgraded.\n","Need to get 4,795 kB of archives.\n","After this operation, 15.8 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB]\n","Fetched 4,795 kB in 1s (4,659 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package tesseract-ocr-eng.\n","(Reading database ... 160980 files and directories currently installed.)\n","Preparing to unpack .../tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ...\n","Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n","Selecting previously unselected package tesseract-ocr-osd.\n","Preparing to unpack .../tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ...\n","Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n","Selecting previously unselected package tesseract-ocr.\n","Preparing to unpack .../tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ...\n","Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ...\n","Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n","Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n","Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Collecting pytesseract\n","  Downloading https://files.pythonhosted.org/packages/a0/e6/a4e9fc8a93c1318540e8de6d8d4beb5749b7960388a7c7f27799fc2dd016/pytesseract-0.3.7.tar.gz\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pytesseract) (7.0.0)\n","Building wheels for collected packages: pytesseract\n","  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytesseract: filename=pytesseract-0.3.7-py2.py3-none-any.whl size=13945 sha256=8bc72e18cb7b3ceacdcf068dd7b4cd15a2fa90a474bf4dcfb7d07081fd76db77\n","  Stored in directory: /root/.cache/pip/wheels/81/20/7e/1dd0daad1575d5260916bb1e9781246430647adaef4b3ca3b3\n","Successfully built pytesseract\n","Installing collected packages: pytesseract\n","Successfully installed pytesseract-0.3.7\n","Collecting tesseract\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/b7/c4fae9af5842f69d9c45bf1195a94aec090628535c102894552a7a7dbe6c/tesseract-0.1.3.tar.gz (45.6MB)\n","\u001b[K     |████████████████████████████████| 45.6MB 73kB/s \n","\u001b[?25hBuilding wheels for collected packages: tesseract\n","  Building wheel for tesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tesseract: filename=tesseract-0.1.3-cp37-none-any.whl size=45562571 sha256=56c271900b172a6b2f502f8618d1be75019b81bbc10e07dc6d506bd2b8995645\n","  Stored in directory: /root/.cache/pip/wheels/82/1f/d9/24797b123379e4ea9511cf660835468b62dad609634cad2aba\n","Successfully built tesseract\n","Installing collected packages: tesseract\n","Successfully installed tesseract-0.1.3\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following NEW packages will be installed:\n","  poppler-utils\n","0 upgraded, 1 newly installed, 0 to remove and 30 not upgraded.\n","Need to get 154 kB of archives.\n","After this operation, 613 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 poppler-utils amd64 0.62.0-2ubuntu2.12 [154 kB]\n","Fetched 154 kB in 0s (313 kB/s)\n","Selecting previously unselected package poppler-utils.\n","(Reading database ... 161027 files and directories currently installed.)\n","Preparing to unpack .../poppler-utils_0.62.0-2ubuntu2.12_amd64.deb ...\n","Unpacking poppler-utils (0.62.0-2ubuntu2.12) ...\n","Setting up poppler-utils (0.62.0-2ubuntu2.12) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P9SASZV5FzaI","executionInfo":{"status":"ok","timestamp":1616430045617,"user_tz":-480,"elapsed":1239,"user":{"displayName":"Axel Mukwena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkBUMeSLjKE8qwDBLKGkDOZkIiuxwvNO8t-yPMcPM=s64","userId":"13593993050630757383"}}},"source":["from pdf2image import convert_from_path\n","import cv2\n","import os"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKfATwzPJVzx","executionInfo":{"status":"ok","timestamp":1616430047139,"user_tz":-480,"elapsed":833,"user":{"displayName":"Axel Mukwena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkBUMeSLjKE8qwDBLKGkDOZkIiuxwvNO8t-yPMcPM=s64","userId":"13593993050630757383"}},"outputId":"898fa558-c7d7-4e02-9579-7679ca63c9b7"},"source":["!ls"],"execution_count":8,"outputs":[{"output_type":"stream","text":[" 100.pdf\t       dummy.pdf   oshengBook.pdf   processed_pages\n","'Copy of page_1.jpg'   ocr.ipynb   pages\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hniJ_W4nKYM5","collapsed":true,"executionInfo":{"status":"ok","timestamp":1616428482020,"user_tz":-480,"elapsed":118792,"user":{"displayName":"Axel Mukwena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkBUMeSLjKE8qwDBLKGkDOZkIiuxwvNO8t-yPMcPM=s64","userId":"13593993050630757383"}},"outputId":"bb674a35-f97c-41b1-b19e-0c2ceba8c328"},"source":["pdf = 'oshengBook.pdf'\n","pages = convert_from_path(pdf)\n","\n","if not os.path.exists(\"pages\"):\n","  os.makedirs(\"pages\")\n","\n","i, j = 1, 1\n","for page in pages:\n","\n","  # Skip the useless pages\n","  #if j < 16 or j > 124:\n","    #i -= 1\n","    #continue\n","   \n","  image_name = \"pages/\" + str(i) + \".jpg\"  \n","  page.save(image_name, \"JPEG\")\n","\n","  print(\"Page:\", i)\n","  i += 1\n","  j += 1"],"execution_count":302,"outputs":[{"output_type":"stream","text":["Page:  1\n","Page:  2\n","Page:  3\n","Page:  4\n","Page:  5\n","Page:  6\n","Page:  7\n","Page:  8\n","Page:  9\n","Page:  10\n","Page:  11\n","Page:  12\n","Page:  13\n","Page:  14\n","Page:  15\n","Page:  16\n","Page:  17\n","Page:  18\n","Page:  19\n","Page:  20\n","Page:  21\n","Page:  22\n","Page:  23\n","Page:  24\n","Page:  25\n","Page:  26\n","Page:  27\n","Page:  28\n","Page:  29\n","Page:  30\n","Page:  31\n","Page:  32\n","Page:  33\n","Page:  34\n","Page:  35\n","Page:  36\n","Page:  37\n","Page:  38\n","Page:  39\n","Page:  40\n","Page:  41\n","Page:  42\n","Page:  43\n","Page:  44\n","Page:  45\n","Page:  46\n","Page:  47\n","Page:  48\n","Page:  49\n","Page:  50\n","Page:  51\n","Page:  52\n","Page:  53\n","Page:  54\n","Page:  55\n","Page:  56\n","Page:  57\n","Page:  58\n","Page:  59\n","Page:  60\n","Page:  61\n","Page:  62\n","Page:  63\n","Page:  64\n","Page:  65\n","Page:  66\n","Page:  67\n","Page:  68\n","Page:  69\n","Page:  70\n","Page:  71\n","Page:  72\n","Page:  73\n","Page:  74\n","Page:  75\n","Page:  76\n","Page:  77\n","Page:  78\n","Page:  79\n","Page:  80\n","Page:  81\n","Page:  82\n","Page:  83\n","Page:  84\n","Page:  85\n","Page:  86\n","Page:  87\n","Page:  88\n","Page:  89\n","Page:  90\n","Page:  91\n","Page:  92\n","Page:  93\n","Page:  94\n","Page:  95\n","Page:  96\n","Page:  97\n","Page:  98\n","Page:  99\n","Page:  100\n","Page:  101\n","Page:  102\n","Page:  103\n","Page:  104\n","Page:  105\n","Page:  106\n","Page:  107\n","Page:  108\n","Page:  109\n","Page:  110\n","Page:  111\n","Page:  112\n","Page:  113\n","Page:  114\n","Page:  115\n","Page:  116\n","Page:  117\n","Page:  118\n","Page:  119\n","Page:  120\n","Page:  121\n","Page:  122\n","Page:  123\n","Page:  124\n","Page:  125\n","Page:  126\n","Page:  127\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5vHXtTws0ezO","executionInfo":{"status":"ok","timestamp":1616430056658,"user_tz":-480,"elapsed":3142,"user":{"displayName":"Axel Mukwena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkBUMeSLjKE8qwDBLKGkDOZkIiuxwvNO8t-yPMcPM=s64","userId":"13593993050630757383"}},"outputId":"efe40e56-e54f-469c-94d9-e82dba3e3e28"},"source":["import nltk\n","nltk.download('punkt')\n","from nltk import word_tokenize"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"TSwiCQgZ4bnh","executionInfo":{"status":"ok","timestamp":1616430082726,"user_tz":-480,"elapsed":24335,"user":{"displayName":"Axel Mukwena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkBUMeSLjKE8qwDBLKGkDOZkIiuxwvNO8t-yPMcPM=s64","userId":"13593993050630757383"}},"outputId":"fe654849-80a6-4f0f-ee21-ef68f307584f"},"source":["!apt install enchant\n","!pip install pyenchant\n","!pip install inflect"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n","  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n","Suggested packages:\n","  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n","  | openoffice.org-core libenchant-voikko\n","The following NEW packages will be installed:\n","  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n","  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n","0 upgraded, 10 newly installed, 0 to remove and 30 not upgraded.\n","Need to get 1,310 kB of archives.\n","After this operation, 5,353 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtext-iconv-perl amd64 1.7-5build6 [13.0 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaspell15 amd64 0.60.7~20110707-4ubuntu0.1 [309 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 emacsen-common all 2.0.8 [17.6 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 dictionaries-common all 1.27.2 [186 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 aspell amd64 0.60.7~20110707-4ubuntu0.1 [87.6 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 aspell-en all 2017.08.24-0-0.1 [298 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 hunspell-en-us all 1:2017.08.24 [168 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhunspell-1.6-0 amd64 1.6.2-1 [154 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libenchant1c2a amd64 1.6.0-11.1 [64.4 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 enchant amd64 1.6.0-11.1 [12.2 kB]\n","Fetched 1,310 kB in 1s (1,623 kB/s)\n","Preconfiguring packages ...\n","Selecting previously unselected package libtext-iconv-perl.\n","(Reading database ... 161055 files and directories currently installed.)\n","Preparing to unpack .../0-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n","Unpacking libtext-iconv-perl (1.7-5build6) ...\n","Selecting previously unselected package libaspell15:amd64.\n","Preparing to unpack .../1-libaspell15_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n","Unpacking libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n","Selecting previously unselected package emacsen-common.\n","Preparing to unpack .../2-emacsen-common_2.0.8_all.deb ...\n","Unpacking emacsen-common (2.0.8) ...\n","Selecting previously unselected package dictionaries-common.\n","Preparing to unpack .../3-dictionaries-common_1.27.2_all.deb ...\n","Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n","Unpacking dictionaries-common (1.27.2) ...\n","Selecting previously unselected package aspell.\n","Preparing to unpack .../4-aspell_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n","Unpacking aspell (0.60.7~20110707-4ubuntu0.1) ...\n","Selecting previously unselected package aspell-en.\n","Preparing to unpack .../5-aspell-en_2017.08.24-0-0.1_all.deb ...\n","Unpacking aspell-en (2017.08.24-0-0.1) ...\n","Selecting previously unselected package hunspell-en-us.\n","Preparing to unpack .../6-hunspell-en-us_1%3a2017.08.24_all.deb ...\n","Unpacking hunspell-en-us (1:2017.08.24) ...\n","Selecting previously unselected package libhunspell-1.6-0:amd64.\n","Preparing to unpack .../7-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n","Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n","Selecting previously unselected package libenchant1c2a:amd64.\n","Preparing to unpack .../8-libenchant1c2a_1.6.0-11.1_amd64.deb ...\n","Unpacking libenchant1c2a:amd64 (1.6.0-11.1) ...\n","Selecting previously unselected package enchant.\n","Preparing to unpack .../9-enchant_1.6.0-11.1_amd64.deb ...\n","Unpacking enchant (1.6.0-11.1) ...\n","Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n","Setting up libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n","Setting up emacsen-common (2.0.8) ...\n","Setting up libtext-iconv-perl (1.7-5build6) ...\n","Setting up dictionaries-common (1.27.2) ...\n","Setting up aspell (0.60.7~20110707-4ubuntu0.1) ...\n","Setting up hunspell-en-us (1:2017.08.24) ...\n","Setting up libenchant1c2a:amd64 (1.6.0-11.1) ...\n","Setting up aspell-en (2017.08.24-0-0.1) ...\n","Setting up enchant (1.6.0-11.1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for dictionaries-common (1.27.2) ...\n","aspell-autobuildhash: processing: en [en-common].\n","aspell-autobuildhash: processing: en [en-variant_0].\n","aspell-autobuildhash: processing: en [en-variant_1].\n","aspell-autobuildhash: processing: en [en-variant_2].\n","aspell-autobuildhash: processing: en [en-w_accents-only].\n","aspell-autobuildhash: processing: en [en-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_AU-variant_0].\n","aspell-autobuildhash: processing: en [en_AU-variant_1].\n","aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n","aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_CA-variant_0].\n","aspell-autobuildhash: processing: en [en_CA-variant_1].\n","aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n","aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-variant_0].\n","aspell-autobuildhash: processing: en [en_GB-variant_1].\n","aspell-autobuildhash: processing: en [en_US-w_accents-only].\n","aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n","Collecting pyenchant\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/8c/bd224a5db562ac008edbfaf015f5d5c98ea13e745247cd4ab5fc5b683085/pyenchant-3.2.0-py3-none-any.whl (55kB)\n","\u001b[K     |████████████████████████████████| 61kB 3.0MB/s \n","\u001b[?25hInstalling collected packages: pyenchant\n","Successfully installed pyenchant-3.2.0\n","Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (2.1.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3bP1sw60bKu6","executionInfo":{"status":"ok","timestamp":1616430095718,"user_tz":-480,"elapsed":752,"user":{"displayName":"Axel Mukwena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkBUMeSLjKE8qwDBLKGkDOZkIiuxwvNO8t-yPMcPM=s64","userId":"13593993050630757383"}}},"source":["from google.colab.patches import cv2_imshow\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","import pytesseract\n","import numpy as np\n","import re\n","import inflect\n","\n","p = inflect.engine()"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SJ7ZhECN44K-","executionInfo":{"status":"ok","timestamp":1616430098631,"user_tz":-480,"elapsed":907,"user":{"displayName":"Axel Mukwena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkBUMeSLjKE8qwDBLKGkDOZkIiuxwvNO8t-yPMcPM=s64","userId":"13593993050630757383"}},"outputId":"ba673384-3b17-4017-dd79-8affc3832a7a"},"source":["import enchant\n","print(enchant.list_languages())\n","d = enchant.Dict(\"en_GB\")\n","print(d.check(\"Hello\"))\n","print(d.check(\"ye\"))\n","print(d.check(\"honeybadger\"))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["['en_US', 'en', 'en_AU', 'en_CA', 'en_GB']\n","True\n","True\n","False\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"66UXUPqVcRAL","executionInfo":{"status":"ok","timestamp":1616430552417,"user_tz":-480,"elapsed":451432,"user":{"displayName":"Axel Mukwena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkBUMeSLjKE8qwDBLKGkDOZkIiuxwvNO8t-yPMcPM=s64","userId":"13593993050630757383"}},"outputId":"278fc8b1-b867-4549-e6b4-83c402ec6c27"},"source":["# load the images\n","directory = os.listdir('pages/toOshi')\n","images = sorted(directory, key=lambda x: int(os.path.splitext(x)[0]))\n","\n","for i in images:\n","  if i.endswith('.jpg'):\n","    im = cv2.imread(\"pages/toOshi/\" + i)\n","    im_copy = cv2.imread(\"pages/toOshi/\" + i)\n","\n","    # https://alloyui.com/examples/color-picker/hsv.html\n","    hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n","    lower_gray = np.array([0, 0, 0])\n","    upper_gray = np.array([179,50, 140])\n","    mask_gray = cv2.inRange(hsv, lower_gray, upper_gray)\n","    img_res = cv2.bitwise_and(im, im, mask = mask_gray)\n","\n","    img_res = cv2.cvtColor(img_res, cv2.COLOR_BGR2GRAY)\n","    ret, thresh = cv2.threshold(img_res, 179,50, 140)\n","\n","    # Dilate to combine adjacent text contours\n","    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9,9))\n","    dilate = cv2.dilate(thresh, kernel, iterations=2)\n","\n","    contours, hierarchy = cv2.findContours(dilate, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","    # output = cv2.drawContours(im, contours, -1, (0,255,0), 2)\n","\n","    # Iterate through all the contours\n","    coordinates = []\n","    for contour in contours:\n","        # Find bounding rectangles\n","        x, y, w, h = cv2.boundingRect(contour)\n","        # Draw the rectangle\n","        if w > 300:\n","          cv2.rectangle(im, (x, y), (x + w, y + h), (255 ,0 , 0), 1)\n","          coordinates.append([(x, y), (x + w, y + h)])\n","  \n","    coordinates.reverse()\n","\n","    new_words = []\n","    for c in coordinates:\n","      # cropping image img = image[y0:y1, x0:x1]\n","      img = im_copy[c[0][1]:c[1][1], c[0][0]:c[1][0]]    \n","      \n","      # Grayscale, Gaussian blur\n","      gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","      blur = cv2.GaussianBlur(gray, (3,3), 0)\n","      \n","      # Perform text extraction\n","      text = pytesseract.image_to_string(blur, lang='eng', config='--psm 6')\n","      \n","      text = text.lower()\n","      text = text.replace('\\n', ' ')\n","      text = text.replace('.', '')\n","      text = text.replace('?', '')\n","      text = text.replace('!', '')\n","      text = text.replace('0', 'o')\n","      text = text.replace('(', '')\n","      text = text.replace(')', '')\n","      text = text.replace(',', '')\n","      text = text.replace(':', '')\n","      # Wacth out the ’ apostrophe\n","      text = text.replace('roberts’', '')\n","      text = re.sub('\\d', '', text)\n","      words = word_tokenize(text)\n","\n","      english = []\n","      n = len(words)\n","\n","      for j in range(n):\n","        nn = len(new_words)\n","        ee = len(english)\n","\n","        if words[j] == \"ye\":\n","          words[j] == \"kalipi\"\n","\n","        if words[j] is \"#\":\n","          try:\n","            if not d.check(new_words[nn - 1]):\n","              new_words.append(\"\\n\")\n","            new_words.append(english[ee - 1])\n","          except:\n","            pass\n","          continue\n","        if d.check(words[j]):\n","          new_words.append(words[j])\n","          english.append(words[j])\n","          try:\n","            if d.check(words[j + 1]):\n","              continue\n","            else:\n","              if words[j + 1] is \"#\":\n","                continue\n","              else:\n","                new_words.append(\"\\n\")\n","          except:\n","            pass\n","            \n","        elif not d.check(words[j]):\n","          \n","          # \"ee-\" does not exist in oshiwambo, lol\n","          items = [\"aa-\", \"ii-\", \"oo-\", \"uu-\", \n","                   \"oma-\", \"omi-\", \"omau-\", \"omalu-\"]\n","          \n","          if words[j] in items:\n","            # when have more than one pluralize\n","            if words[j - 1] in items:\n","              continue\n","            \n","            # Add plural version of last english word \n","            try:  \n","              new_words.append(\"\\n\")\n","              p_word = p.plural(english[ee - 1]) \n","              new_words.append(p_word)\n","              new_words.append(\"\\n\")\n","            except:\n","              pass\n","\n","            try:\n","              # remove \"omu\"\n","              if words[j] == \"aa-\":\n","                words[j] = \"aa\" + words[j - 1][3:]\n","              \n","              # remove \"oshi\"\n","              if words[j] == \"ii-\":\n","                words[j] = \"ii\" + words[j - 1][4:]\n","\n","              # remove \"o\"\n","              if words[j] == \"oo-\":\n","                words[j] = \"oo\" + words[j - 1][1:]\n","\n","              # remove \"oka\"\n","              if words[j] == \"uu-\":\n","                words[j] = \"uu\" + words[j - 1][3:]\n","\n","              # remove \"e\"\n","              if words[j] == \"oma-\":\n","                words[j] = \"oma\" + words[j - 1][1:]\n","\n","              # remove \"omu\"\n","              if words[j] == \"omi-\":\n","                words[j] = \"omi\" + words[j - 1][3:]\n","\n","              # remove \"uu\"\n","              if words[j] == \"omau-\":\n","                words[j] = \"omau\" + words[j - 1][2:]\n","\n","              # remove \"olu\"\n","              if words[j] == \"omalu-\":\n","                words[j] = \"omalu\" + words[j - 1][3:]\n","            except:\n","              pass\n","\n","          try:\n","            if new_words[nn - 1] is \"#\":\n","              new_words.append(\"\\n\")\n","          except:\n","            pass\n","\n","          w = words[j].replace(\"c\", \"e\")\n","          if words[j] == \"kalipi\":\n","            new_words.append(\"ye\")\n","          else:\n","            new_words.append(w)\n","\n","          try:\n","            if not d.check(words[j + 1]) or words[j + 1] == \"ye\":\n","              continue\n","            else:\n","              new_words.append(\"\\n\")\n","          except:\n","            pass\n","\n","    # print(new_words)\n","    sentence = \" \".join(new_words)\n","    sentence = sentence.splitlines()\n","    ss = [word.strip() for word in sentence]\n","    ss = \"\\n\".join(ss)\n","    # print(ss)\n","\n","    if not os.path.exists(\"processed_pages\"):\n","      os.makedirs(\"processed_pages\")\n","\n","    title = i.replace('.jpg', '')\n","    with open(\"processed_pages/\" + title + '.txt', 'w') as f:\n","      f.write(ss)\n","      f.close()\n","\n","    print(title)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","100\n","101\n","102\n","103\n","104\n","105\n","106\n","107\n","108\n","109\n","110\n","111\n","112\n","113\n","114\n","115\n","116\n","117\n","118\n","119\n","120\n","121\n","122\n","123\n","124\n"],"name":"stdout"}]}]}